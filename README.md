# ğŸŒ Cross-Language Information Retrieval (CLIR) System

A comprehensive system that retrieves relevant documents or information in one language based on a query entered in another language using NLP and Translation APIs.

## ğŸ“‹ Project Overview

This project implements a Cross-Language Information Retrieval system that:
- Accepts queries in multiple languages (Hindi, Tamil, Telugu, Bengali, etc.)
- Translates queries to English automatically
- Retrieves relevant documents from an English corpus using TF-IDF and Cosine Similarity
- Returns results in both English and translated target language

### Example Use Case
**Input (Hindi):** "à¤­à¤¾à¤°à¤¤ à¤•à¤¾ à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤•à¥Œà¤¨ à¤¹à¥ˆ?"  
**Translated:** "Who is the Prime Minister of India?"  
**Output:** Relevant documents about the Prime Minister of India in both English and Hindi

## ğŸš€ Features

- âœ… Multi-language query support (Hindi, English, Tamil, Telugu, Bengali, and more)
- âœ… Automatic language detection and translation
- âœ… TF-IDF vectorization for document indexing
- âœ… Cosine similarity for relevance ranking
- âœ… Interactive Streamlit UI with visualizations
- âœ… Translation caching to reduce API calls
- âœ… Comprehensive evaluation metrics (Precision, Recall, F1-score)
- âœ… Command-line interface for batch processing
- âœ… Logging and query statistics

## ğŸ“ Project Structure

```
CLIR_Project/
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Main entry point (CLI)
â”‚   â”œâ”€â”€ preprocessing.py        # Text preprocessing
â”‚   â”œâ”€â”€ translation.py          # Language translation
â”‚   â”œâ”€â”€ retrieval.py            # TF-IDF retrieval
â”‚   â”œâ”€â”€ evaluation.py           # Performance metrics
â”‚   â”œâ”€â”€ ui.py                   # Streamlit UI
â”‚   â””â”€â”€ utils.py                # Helper functions
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ english_corpus.txt      # English documents corpus
â”‚   â”œâ”€â”€ hindi_corpus.txt        # Optional Hindi corpus
â”‚   â””â”€â”€ stopwords/              # Custom stopword lists
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl    # Saved TF-IDF model
â”‚   â””â”€â”€ translations_cache.json # Translation cache
â”‚
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ data_preparation.ipynb  # Data preprocessing
â”‚   â”œâ”€â”€ model_training.ipynb    # Model training
â”‚   â””â”€â”€ evaluation.ipynb        # Performance analysis
â”‚
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ project_report.docx     # Project documentation
â”‚   â”œâ”€â”€ ppt_presentation.pptx   # Presentation slides
â”‚   â””â”€â”€ readme_images/          # Screenshots
â”‚
â”œâ”€â”€ ğŸ“ results/
â”‚   â”œâ”€â”€ sample_queries.json     # Example queries
â”‚   â””â”€â”€ performance_metrics.csv # Evaluation results
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Setup Steps

1. **Clone or download the project**
   ```bash
   cd CLIR_Project
   ```

2. **Create a virtual environment (recommended)**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download NLTK data** (if not automatically downloaded)
   ```python
   import nltk
   nltk.download('punkt')
   nltk.download('stopwords')
   nltk.download('wordnet')
   nltk.download('averaged_perceptron_tagger')
   ```

## ğŸ¯ Usage

### 1. Command-Line Interface

Run the main script for interactive CLI:
```bash
python -m src.main
```

Or:
```bash
python src/main.py
```

### 2. Streamlit Web UI

Launch the interactive web interface:
```bash
streamlit run src/ui.py
```

The UI will open in your default browser at `http://localhost:8501`

### 3. Using as a Python Module

```python
from src.translation import translate_query
from src.retrieval import retrieve_documents

# Translate and retrieve
query = "à¤­à¤¾à¤°à¤¤ à¤•à¤¾ à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤•à¥Œà¤¨ à¤¹à¥ˆ?"
translated = translate_query(query, src='auto', dest='en')
results = retrieve_documents(translated, top_k=5)

for doc, score in results:
    print(f"Score: {score:.3f} - {doc}")
```

## ğŸ“Š Evaluation

Run evaluation on test queries:
```python
from src.evaluation import RetrievalEvaluator

evaluator = RetrievalEvaluator()

# Example: Evaluate a query
metrics = evaluator.evaluate_query(
    query="Who is the Prime Minister of India?",
    relevant_doc_indices=[1, 2],  # Indices of relevant documents
    top_k=5
)

print(metrics)
```

## ğŸ”§ Configuration

The system uses default settings, but you can customize:
- **Corpus path**: Modify `corpus_path` in `retrieval.py`
- **Translation cache**: Automatically saved in `models/translations_cache.json`
- **TF-IDF model**: Saved in `models/tfidf_vectorizer.pkl`

## ğŸ“ˆ Performance Metrics

The system evaluates retrieval using:
- **Precision@K**: Fraction of retrieved documents that are relevant
- **Recall@K**: Fraction of relevant documents that are retrieved
- **F1-Score@K**: Harmonic mean of Precision and Recall
- **MRR**: Mean Reciprocal Rank

## ğŸŒ Supported Languages

The system supports translation for 100+ languages including:
- Hindi (à¤¹à¤¿à¤‚à¤¦à¥€)
- Tamil (à®¤à®®à®¿à®´à¯)
- Telugu (à°¤à±†à°²à±à°—à±)
- Bengali (à¦¬à¦¾à¦‚à¦²à¦¾)
- English
- And many more via Google Translate API

## ğŸ¨ UI Features

The Streamlit UI includes:
- ğŸŒ Multi-language query input
- ğŸ“Š Interactive similarity score charts
- ğŸ“‹ List and chart views of results
- ğŸŒ Automatic translation of top results
- âš™ï¸ Customizable settings (number of results, display options)
- ğŸ’¡ Example queries in sidebar

## ğŸ“ Logging

All queries and operations are logged to:
- Console output
- `clir_system.log` file

## ğŸ› Troubleshooting

### Issue: Translation API errors
- **Solution**: Check internet connection. Google Translate API may have rate limits.

### Issue: NLTK data not found
- **Solution**: Run the NLTK download commands mentioned in installation.

### Issue: Import errors
- **Solution**: Ensure you're running from the project root directory and all dependencies are installed.

### Issue: Model not found
- **Solution**: The system will automatically build the TF-IDF model on first run if not found.

## ğŸ”® Future Enhancements

- [ ] Support for more Indian languages
- [ ] BERT multilingual embeddings (mBERT) for improved retrieval
- [ ] Voice input and output (speech-to-text + text-to-speech)
- [ ] Integration with Wikipedia API for dynamic data
- [ ] Advanced UI themes and customization
- [ ] Real-time query suggestions
- [ ] Multi-document summarization

## ğŸ“š References

- TF-IDF: Term Frequency-Inverse Document Frequency
- Cosine Similarity for document matching
- Google Translate API for language translation
- NLTK for natural language processing

## ğŸ‘¥ Contributing

This is an academic project. For improvements or suggestions, please refer to the project documentation.

## ğŸ“„ License

This project is for academic/educational purposes.

## ğŸ™ Acknowledgments

- Google Translate API for translation services
- NLTK and scikit-learn communities
- Streamlit for the web framework

---

**Built with â¤ï¸ for Cross-Language Information Retrieval**

For questions or issues, please refer to the project documentation in the `docs/` folder.

