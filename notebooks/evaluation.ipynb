{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Notebook\n",
        "\n",
        "This notebook demonstrates evaluation metrics for the CLIR system.\n",
        "\n",
        "## Metrics:\n",
        "- Precision@K\n",
        "- Recall@K\n",
        "- F1-Score@K\n",
        "- Mean Reciprocal Rank (MRR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path().resolve().parent))\n",
        "\n",
        "from src.evaluation import RetrievalEvaluator\n",
        "from src.retrieval import DocumentRetriever\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = RetrievalEvaluator()\n",
        "\n",
        "print(f\"Loaded {len(evaluator.retriever.documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Evaluation\n",
        "\n",
        "For demonstration, we'll evaluate a query with known relevant documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Query about Prime Minister\n",
        "# Assuming document at index 1 is relevant\n",
        "query = \"Who is the Prime Minister of India?\"\n",
        "relevant_indices = [1]  # Index of relevant document\n",
        "\n",
        "metrics = evaluator.evaluate_query(query, relevant_indices, top_k=5)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example batch evaluation\n",
        "test_queries = [\n",
        "    (\"Who is the Prime Minister of India?\", [1]),\n",
        "    (\"What is the capital of India?\", [2]),\n",
        "    (\"What is the official language of India?\", [4]),\n",
        "]\n",
        "\n",
        "avg_metrics = evaluator.evaluate_batch(test_queries, top_k=5)\n",
        "\n",
        "print(\"Average Metrics:\")\n",
        "for metric, value in avg_metrics.items():\n",
        "    print(f\"{metric}: {value:.3f}\")\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame([avg_metrics])\n",
        "df.to_csv(\"../results/performance_metrics.csv\", index=False)\n",
        "print(\"\\nMetrics saved to results/performance_metrics.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
